#----------------#
# Learning JAGS  #
#----------------#

# Model definition - series of relations inside a block delimited by curly brackets and preceded by the keyword model

model{ # in BUGS language
  for(i in 1:N){ # Likelihood
    Y[i] ~ dnorm(mu[i], tau)
    mu[i] <- alpha + beta * (x[i]-x.bar)
  }
  x.bar <- mean(x)
  alpha ~ dnorm(.0, 1e-4) # Priors
  beta ~ dnorm  (.0, 1e-4)
  sigma <- 1.0/sqrt(tau)
  tau ~ dgamma(1e-3,1e-3)
}

# Each relation defines a node.
# The node on the left of a relation is defined in terms of other nodes - parent nodes - that appear on the right hand side. 
# together, the nodes form a directed acyclic grpah
# The top level nodes, with no parents, are constant nodes, defined in the model definition (1e-3), or in the data when the model is compiled (x[i])
# Relations can be stochastic (~): representing a random variable in a model
# Relations can be deterministic (<- | =): the value is determined exactly by the values of its parents

#----------------------#
# Steps to run a model #
#----------------------#

# 1. Define the model
# 2. Compilation
# 3. Initialization
        # a. Intial values of model parameters are set
        # b. A RNG is chosen for each parallel chain, and the seed set
              # i. There are four RNGs supplied by the base module in JAGS 
        # c. The samplers chosen for each parameter
              # i. object that acts on a set of parameters and updates them from one iteration to the next. 
# 4. Adaptation and burn-in
        # The MCMC output is divided into two parts: an initial “burn-in” period, which is discarded, and the remainder
        # of the run, in which the output is considered to have converged (sufficiently close) to the target distribution
        # The burn-in period of a JAGS run is therefore the interval between model initialization and the creation of the first monitor.
        # When a model is initialized, it may be in adaptive mode, meaning that the Samplers used by the model may modify their behaviour for increased efficiency.
        # the sequence generated by an adapting sampler is no longer a Markov chain, and is not guaranteed to converge to the target distribution
# 5. Monitoring
        # A monitor in JAGS is an object that records sampled values. The simplest monitor is a trace monitor, which stores the sampled value of a node at each iteration
        # monitors of type “mean” and “variance” defined by the base module store the running mean and variance, respectively, of a given node

# Data values must always be supplied for N, since it is used to index the for loop. 
# Data must also be supplied for x since it is used on the right hand side of the relation 
# defining mu but never defined on the left hand side of a relation
# parameters of the model are the three stochastic nodes for which no data are supplied:
#       alpha, beta and tau. 

library(rjags)
library(coda)
# we first create a model object with the jags.model() function

line_data <- list("x" = c(1, 2, 3, 4, 5), "Y" = c(1, 3, 3, 3, 5), "N"=5)
line_inits <- list(list("alpha" = 3, "beta"= 0, "tau" = 1.5),
                   list("alpha" = 0, "beta" = 1, "tau" = 0.375))

library(rjags)
model <- jags.model("line.bug", data=line_data, inits=line_inits, n.chains=2)

# To get samples from the posterior distribution of the parameters, we use the coda.samples
# function after first using the update function to run the Markov Chain for a burn-in period of 1000 iterations
samples <- coda.samples(model, variable.names=c("alpha", "beta", "sigma"),
                        n.iter=1000)
# To get an informative summary of the samples, we use the summary function:

summary(samples)

# To produce trace plots and density plots for the sampled values
plot(samples)
